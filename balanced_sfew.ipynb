{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Peeyush\\Phd\\domain-adaptation\\da_cv_fer\\fer\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# from torchvision.datasets import ImageFolder # for datasets\n",
    "from data_config import DataConfig\n",
    "from pathlib import Path\n",
    "import zipfile\n",
    "import torchvision.transforms as transforms # transformation with respect to mean, std, 3 channel\n",
    "from torchvision.datasets import ImageFolder # for datasets (reference: Sai's usage)\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from utils import *\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToPILImage\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torchvision.transforms import v2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from facenet_pytorch import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotSFEWCROPBALANCEDDataset(Dataset):\n",
    "    def __init__(self, root, transform = None, crop_at_runtime = False) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.crop_at_runtime = crop_at_runtime \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        if self.crop_at_runtime:\n",
    "            self.mtcnn = MTCNN(image_size=224,device=self.device)#MTCNN(image_size=224).to(device=self.device)\n",
    "        \n",
    "\n",
    "        # dataconfig = DataConfig()\n",
    "        self.transform = transform\n",
    "        self.basic_transform = transforms.Compose([transforms.Resize((224, 224)),\n",
    "                                                   transforms.ToTensor()])\n",
    "        self.image_folder = ImageFolder(root, transform=self.basic_transform)\n",
    "        self.class_labels = self.image_folder.classes\n",
    "        self.to_pil = ToPILImage()\n",
    "        # self.mean_ds = dataconfig.SFEW_mean_ds\n",
    "        # self.std_dev_ds = dataconfig.SFEW_std_dev_ds\n",
    "        self.tranforms_type = None\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_folder)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.image_folder[idx]        \n",
    "        one_hot_label = torch.zeros(len(self.class_labels))\n",
    "        one_hot_label[label]=1\n",
    "        image_name = self.image_folder.imgs[idx][0]\n",
    "        # print(f'  pixel range value = {torch.max(image.view(-1))} | {torch.min(image.view(-1))}')\n",
    "        # print(\"image before \\n\", image)\n",
    "        # self.to_pil(image).show()\n",
    "\n",
    "        if image is None or (torch.all(image == 0).item() == 1):\n",
    "            return torch.zeros((3,224,224)),one_hot_label, image_name\n",
    "        \n",
    "        if self.crop_at_runtime:\n",
    "            image_pil = self.to_pil(image)\n",
    "\n",
    "            # Get cropped and prewhitened image tensor\n",
    "            img_cropped = self.mtcnn(image_pil,device=self.device)#.to(device=self.device)\n",
    "\n",
    "            if img_cropped is None: # case where face is not detected\n",
    "                if self.transform:\n",
    "                    image = self.transform(image_pil)\n",
    "                else:\n",
    "                    image = self.basic_transform(image_pil)\n",
    "\n",
    "                return image,one_hot_label, image_name\n",
    "            else:# Rescale the tensor from the range [-1, 1] to [0, 1]\n",
    "                image_tensor_rescaled = (img_cropped + 1) / 2\n",
    "                # print(f'  pixel range value = {torch.max(image_tensor_rescaled.view(-1))} | {torch.min(image_tensor_rescaled.view(-1))}')\n",
    "                # self.to_pil(image_tensor_rescaled).show()\n",
    "                return image_tensor_rescaled, one_hot_label, image_name\n",
    "        \n",
    "        else:\n",
    "            # print(f'  pixel range value before = {torch.max(image.view(-1))} | {torch.min(image.view(-1))}')\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(self.to_pil(image))\n",
    "            else:\n",
    "                image = self.basic_transform(self.to_pil(image))\n",
    "            # print(f'  pixel range value = {torch.max(image.view(-1))} | {torch.min(image.view(-1))}')\n",
    "\n",
    "            return image, one_hot_label, image_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSFEWCROPBALANCED():\n",
    "    def __init__(self, crop_at_runtime=False) -> None:\n",
    "        # 1. Download data\n",
    "        self.crop_at_runtime = crop_at_runtime\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if not self.crop_at_runtime:\n",
    "            self.mtcnn = MTCNN(image_size=224,device=self.device)#MTCNN(image_size=224).to(device='cpu') # offline changes are on CPU and not GPU\n",
    "            # self.mtcnn = MTCNN(image_size=224) # offline changes are on CPU and not GPU\n",
    "\n",
    "\n",
    "        dataconfig = DataConfig()\n",
    "        print(\"balanced base path \", dataconfig.SFEW_BALANCED_BASE_PATH)\n",
    "        self.BASE_PATH = dataconfig.SFEW_BALANCED_BASE_PATH\n",
    "        self.origin_file_path = dataconfig.GDRIVE_SFEW_BALANCED_FILE_PATH\n",
    "        self.EXTRACT_PATH = dataconfig.SFEW_BALANCED_EXTRACT_PATH\n",
    "        self.SFEW_BALANCED_DATA_PATH = dataconfig.SFEW_BALANCED_BASE_PATH\n",
    "        # self.ZIP_FILE_PATH = dataconfig.SFEW_ZIP_FILE_PATH\n",
    "        self.labels = ['angry','disgust','fear','happy','neutral','sad','surprise']\n",
    "        self.label_matrix = torch.eye(len(self.labels))\n",
    "\n",
    "        self.dict_dataset = {'TRAIN_DIR' : Path(self.SFEW_BALANCED_DATA_PATH,'Train'),\n",
    "                             'VAL_DIR' : Path(self.SFEW_BALANCED_DATA_PATH,'Val')}\n",
    "        \n",
    "        self.dict_crop_dataset = {'CROP_TRAIN_DIR' : Path(self.SFEW_BALANCED_DATA_PATH,'Train_Crop'),\n",
    "                                  'CROP_VAL_DIR': Path(self.SFEW_BALANCED_DATA_PATH,'Val_Crop')}\n",
    "\n",
    "        self.tranforms_type = None\n",
    "\n",
    "        print(f' self.BASE_PATH -{self.BASE_PATH }, \\n self.SFEW_BALANCED_DATA_PATH-{self.SFEW_BALANCED_DATA_PATH} ')\n",
    "        \n",
    "        # 2. Extract data # Not relevant as it is already extracted\n",
    "        self.extract_dataset() \n",
    "\n",
    "        # 3. Creating Dataset Object\n",
    "        self.mean_ds = dataconfig.SFEW_mean_ds \n",
    "        self.std_dev_ds = dataconfig.SFEW_std_dev_ds \n",
    "        self.train_ds, self.val_ds = None, None # initialization\n",
    "        self.train_ds, self.val_ds = self.get_dataset() #(self.mean_ds,self.std_dev_ds)\n",
    "        \n",
    "        # 4. Creating Dataloader Object\n",
    "        self.BATCH_SIZE = dataconfig.SFEW_BALANCED_BATCH_SIZE\n",
    "        self.cuda = dataconfig.cuda\n",
    "        self.train_dl, self.val_dl = self.get_dataloader()\n",
    "        return\n",
    "\n",
    "    def extract_dataset(self):\n",
    "\n",
    "        # Creating extract directory\n",
    "        if not self.EXTRACT_PATH.exists():\n",
    "            # Create the directory\n",
    "            self.EXTRACT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "            print(f'Directory {self.EXTRACT_PATH} created successfully.')\n",
    "        else:\n",
    "            print(f'Directory {self.EXTRACT_PATH} already exists.')\n",
    "        \n",
    "        # Open the zip if files are not unzipped before\n",
    "        if len(list(self.EXTRACT_PATH.glob(\"*\"))) > 0:\n",
    "            print(f\"Files exist in {self.EXTRACT_PATH}, extraction not done\")\n",
    "        else:\n",
    "            # copy the zip file, as nothing exists\n",
    "            print(f\"No files (including zip file) found in {self.EXTRACT_PATH}.Copying file\")\n",
    "            copy_file(self.origin_file_path,self.EXTRACT_PATH)\n",
    "        \n",
    "        # extract the files if not already present\n",
    "        extract_zip_files(self.EXTRACT_PATH, self.EXTRACT_PATH)\n",
    "                \n",
    "        \n",
    "        if not self.crop_at_runtime:\n",
    "            flag_create_crop_contents = False\n",
    "            for dir_name, dir_path in self.dict_crop_dataset.items():\n",
    "                if not os.path.exists(dir_path): # check if the directories are already present under sfew\n",
    "                    create_directory(dir_path) # creates if not there\n",
    "                    flag_create_crop_contents = True\n",
    "\n",
    "                else:\n",
    "                    if is_directory_empty(dir_path): # check for contents inside them, if contents then exists else print that nothing in crop directory\n",
    "                        print(f'**** {dir_name}/{dir_path} is empty***')\n",
    "                        flag_create_crop_contents = True\n",
    "\n",
    "            if flag_create_crop_contents:\n",
    "                self.create_crop_contents()\n",
    "\n",
    "\n",
    "        return \n",
    "\n",
    "    def create_crop_contents(self):\n",
    "        for dir_name, dir_path in self.dict_dataset.items():\n",
    "            if 'TEST' in dir_name:\n",
    "                pass # no treatment for test directory\n",
    "            else:\n",
    "                crop_dir_name = None      \n",
    "                for key in self.dict_crop_dataset:\n",
    "                    if str(dir_name) in key:\n",
    "                        crop_dir_name = f'CROP_{dir_name}'\n",
    "                \n",
    "                    \n",
    "                if crop_dir_name:\n",
    "                    # for each file in dir_path, do the treatment and store in approrpiate directory\n",
    "                    print(\"dir_path:\", dir_path, \"and subdir: \", os.path.join(dir_path,subdir))\n",
    "\n",
    "                    list_subdir = [ os.path.join(dir_path,subdir) for subdir in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path,subdir))]\n",
    "                    # print(list_subdir)\n",
    "                    for subdir in list_subdir:\n",
    "                        image_file_names = [f for f in os.listdir(subdir) if os.path.isfile(os.path.join(subdir, f))]\n",
    "                        target_dir = os.path.join(self.dict_crop_dataset[crop_dir_name],\n",
    "                                                         os.path.basename(subdir))\n",
    "                        if os.path.exists(target_dir):\n",
    "                            print(f'***No files cropped for { os.path.basename(subdir)}, it is assumed to have files already')\n",
    "                        else:\n",
    "                            for image_name in image_file_names:\n",
    "                                img = Image.open(os.path.join(subdir, image_name)).convert(\"RGB\")\n",
    "                                img_save_path = os.path.join(target_dir,\n",
    "                                                            image_name)\n",
    "                                # print(f'{os.path.join(subdir, image_name)} || {image_name} || {img_save_path}')\n",
    "                                img_cropped = self.mtcnn(img, save_path = img_save_path) #.to(device=self.device)\n",
    "                        \n",
    "                            print(f'{len(image_file_names)} cropped images created in {os.path.basename(subdir)}')\n",
    "\n",
    "    def create_dataset(self, mean_ds = None, std_dev_ds=None):\n",
    "        if mean_ds is None or std_dev_ds is None:\n",
    "            # imagenet data values as default\n",
    "            mean_ds = [0.485, 0.456, 0.406] \n",
    "            std_dev_ds = [0.229, 0.224, 0.225]\n",
    "    \n",
    "\n",
    "        # Train Phase transformations\n",
    "        #TODO: Use albumentations in later versions, first iteration does not include any transformations\n",
    "        print(f'----------mean_ds = {mean_ds}, std_dev_ds = {std_dev_ds}----------')\n",
    "        if self.tranforms_type == 'A': # Albumentations based\n",
    "            sfew_train_transforms = A.Compose([\n",
    "                A.Resize(224,224),# Resize the image to a specific size while maintaining the aspect ratio\n",
    "                A.HorizontalFlip(p=0.7),# Apply horizontal flip with a probability of 50%\n",
    "                A.Rotate(limit =15, p=0.7), # Apply a random rotation between +/- 7 degrees with 50% probability\n",
    "                # A.GaussNoise( p=0.2), # Apply noise\n",
    "                # A.RandomBrightnessContrast(p=0.5),# Random brigtness and Contrast\n",
    "                # A.Normalize(mean=mean_ds, std=std_dev_ds),  # Normalize with calculated mean and std\n",
    "                ToTensorV2(p=1.0) # Convert the image to a PyTorch tensor       \n",
    "            ])\n",
    "        else:\n",
    "            sfew_train_transforms = transforms.Compose([\n",
    "                                        # transforms.CenterCrop(size = (224,224)),\n",
    "                                        transforms.Resize((224, 224)),\n",
    "                                        transforms.RandomApply([transforms.RandomResizedCrop(size=(224,224),scale=(0.8,1.0))],p=0.7),  \n",
    "                                        transforms.RandomApply([transforms.RandomHorizontalFlip(p=0.7)]),  # Horizontal flip with 70% probability\n",
    "                                        transforms.RandomApply([transforms.RandomRotation(degrees=(-15, 15),fill=(1,))], p=0.7),  # Random rotation with 70% probability\n",
    "                                        transforms.RandomApply([transforms.Grayscale(num_output_channels = 3)], p=0.3) , # gray scale\n",
    "                                        transforms.RandomApply([v2.ColorJitter(brightness=.5, hue=.3)], p=0.3) , # color jitter\n",
    "                                        transforms.RandomApply([v2.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5.))], p=0.3) , # gaussian blur\n",
    "                                        transforms.RandomApply([v2.RandomAdjustSharpness(sharpness_factor=2)], p=0.3) , # sharpness\n",
    "                                        transforms.RandomApply([v2.RandomAutocontrast()], p=0.3) , # autocontrast\n",
    "                                        transforms.RandomApply([v2.RandomEqualize()], p=0.3) , # equalize\n",
    "                                        cutout(mask_size=24,p=0.9,cutout_inside=False, mask_color=(255,255,255)) , # cut out white\n",
    "                                        cutout(mask_size=24,p=0.9,cutout_inside=False, mask_color=(255,255,255)) , # cut out white\n",
    "                                        cutout(mask_size=24,p=0.9,cutout_inside=False, mask_color=(255,255,255)) , # cut out white\n",
    "                                        cutout(mask_size=24,p=0.9,cutout_inside=False, mask_color=(255,255,255)) , # cut out white\n",
    "                                        cutout(mask_size=24,p=0.9,cutout_inside=False, mask_color=(0,0,0)), # cut out black\n",
    "                                        cutout(mask_size=24,p=0.9,cutout_inside=False, mask_color=(0,0,0)), # cut out black\n",
    "                                        cutout(mask_size=24,p=0.9,cutout_inside=False, mask_color=(0,0,0)), # cut out black\n",
    "                                        cutout(mask_size=24,p=0.9,cutout_inside=False, mask_color=(0,0,0)), # cut out black\n",
    "                                        transforms.ToTensor(),\n",
    "                                        # transforms.Normalize(mean_ds, std_dev_ds)\n",
    "                                        ])\n",
    "\n",
    "        # Val Phase transformations\n",
    "        if self.tranforms_type == 'A': # Albumentations based\n",
    "            sfew_val_transforms=A.Compose([\n",
    "                A.Resize(224,224),# Resize the image to a specific size while maintaining the aspect ratio\n",
    "                # A.Normalize(mean=mean_ds, std=std_dev_ds),  # Normalize with calculated mean and std\n",
    "                ToTensorV2() # Convert the image to a PyTorch tensor\n",
    "            ])\n",
    "        else:\n",
    "            sfew_val_transforms = transforms.Compose([\n",
    "                                                # transforms.CenterCrop(size = (224,224)),\n",
    "                                                transforms.Resize((224, 224)),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                # transforms.Normalize(mean_ds, std_dev_ds)\n",
    "                                                ])\n",
    "        if self.crop_at_runtime:\n",
    "            sfew_train_ds = OneHotSFEWCROPBALANCEDDataset(root=self.dict_dataset['TRAIN_DIR'],\n",
    "                                    transform=sfew_train_transforms,crop_at_runtime = True)\n",
    "            sfew_val_ds = OneHotSFEWCROPBALANCEDDataset(root=self.dict_dataset['VAL_DIR'],\n",
    "                                   transform=sfew_val_transforms, crop_at_runtime = True)\n",
    "        else:\n",
    "            sfew_train_ds = OneHotSFEWCROPBALANCEDDataset(root=self.dict_crop_dataset['CROP_TRAIN_DIR'],\n",
    "                                    transform=sfew_train_transforms,\n",
    "                                    crop_at_runtime = False)\n",
    "        \n",
    "            sfew_val_ds = OneHotSFEWCROPBALANCEDDataset(root=self.dict_crop_dataset['CROP_VAL_DIR'],\n",
    "                                   transform=sfew_val_transforms,\n",
    "                                   crop_at_runtime = False)\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "        return sfew_train_ds, sfew_val_ds\n",
    "    \n",
    "    def get_dataset(self, mean_ds = None, std_dev_ds=None):\n",
    "        if self.train_ds is None and self.val_ds is None:\n",
    "            if self.mean_ds is None and self.std_dev_ds is None:\n",
    "               return self.create_dataset(mean_ds=self.mean_ds,std_dev_ds=self.std_dev_ds)\n",
    "            else:\n",
    "                return self.create_dataset(mean_ds=mean_ds,std_dev_ds=std_dev_ds)\n",
    "\n",
    "\n",
    "        return self.train_ds, self.val_ds\n",
    "\n",
    "    def get_dataloader(self,BATCH_SIZE=None):\n",
    "        if BATCH_SIZE is not None:\n",
    "            self.BATCH_SIZE = BATCH_SIZE\n",
    "        \n",
    "        dataloader_args = dict(shuffle=True, batch_size=self.BATCH_SIZE, num_workers=4, pin_memory=True) if self.cuda else dict(shuffle=True, batch_size=self.BATCH_SIZE)\n",
    "\n",
    "        if self.train_ds is None or self.val_ds is None:\n",
    "            # self.train_ds, self.val_ds = self.get_dataset(mean_ds=self.mean_ds,std_dev_ds=self.std_dev_ds)\n",
    "            self.train_ds, self.val_ds = self.create_dataset()\n",
    "\n",
    "        # train dataloader\n",
    "        sfew_train_loader = DataLoader(self.train_ds, **dataloader_args)\n",
    "\n",
    "        # val dataloader\n",
    "        sfew_val_loader = DataLoader(self.val_ds, **dataloader_args)\n",
    "\n",
    "        return sfew_train_loader, sfew_val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced base path  dataset\n",
      " self.BASE_PATH -dataset, \n",
      " self.SFEW_BALANCED_DATA_PATH-dataset \n",
      "Directory dataset\\balanced_SFEW already exists.\n",
      "Files exist in dataset\\balanced_SFEW, extraction not done\n",
      "Unzipped Files already exist in dataset\\balanced_SFEW, not extracted\n",
      "**** CROP_TRAIN_DIR/dataset\\Train_Crop is empty***\n",
      "**** CROP_VAL_DIR/dataset\\Val_Crop is empty***\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'dataset\\\\Train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sfew \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetSFEWCROPBALANCED\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_at_runtime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m sfew_train_loader, sfew_val_loader \u001b[38;5;241m=\u001b[39m sfew\u001b[38;5;241m.\u001b[39mget_dataloader()\n\u001b[0;32m      5\u001b[0m utils\u001b[38;5;241m.\u001b[39mshow_batch(sfew_train_loader,sfew\u001b[38;5;241m.\u001b[39mlabels,\u001b[38;5;241m4\u001b[39m,normalized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[3], line 31\u001b[0m, in \u001b[0;36mDatasetSFEWCROPBALANCED.__init__\u001b[1;34m(self, crop_at_runtime)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m self.BASE_PATH -\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBASE_PATH\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m self.SFEW_BALANCED_DATA_PATH-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSFEW_BALANCED_DATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# 2. Extract data # Not relevant as it is already extracted\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 3. Creating Dataset Object\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_ds \u001b[38;5;241m=\u001b[39m dataconfig\u001b[38;5;241m.\u001b[39mSFEW_mean_ds \n",
      "Cell \u001b[1;32mIn[3], line 80\u001b[0m, in \u001b[0;36mDatasetSFEWCROPBALANCED.extract_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 flag_create_crop_contents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flag_create_crop_contents:\n\u001b[1;32m---> 80\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_crop_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 97\u001b[0m, in \u001b[0;36mDatasetSFEWCROPBALANCED.create_crop_contents\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     93\u001b[0m         crop_dir_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCROP_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m crop_dir_name:\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;66;03m# for each file in dir_path, do the treatment and store in approrpiate directory\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m     list_subdir \u001b[38;5;241m=\u001b[39m [ os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path,subdir) \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dir_path,subdir))]\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# print(list_subdir)\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m subdir \u001b[38;5;129;01min\u001b[39;00m list_subdir:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'dataset\\\\Train'"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import os\n",
    "sfew = DatasetSFEWCROPBALANCED(crop_at_runtime=False)\n",
    "sfew_train_loader, sfew_val_loader = sfew.get_dataloader()\n",
    "utils.show_batch(sfew_train_loader,sfew.labels,4,normalized=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
